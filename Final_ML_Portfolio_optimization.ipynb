{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final ML Portfolio optimization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFDw-u-XLwlb",
        "outputId": "b59d16b2-2a7c-4daa-bcdf-517a221a18ba"
      },
      "source": [
        "! pip install ratelimit\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from urllib.request import urlopen\n",
        "\n",
        "!pip install pytrends\n",
        "!pip install fredapi\n",
        "!pip install quandl\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import linregress\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pickle\n",
        "from pandas.tseries.offsets import BMonthEnd\n",
        "import json\n",
        "import lightgbm as lgb\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from scipy.stats import norm, kurtosis, skew\n",
        "import math\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "import scipy.optimize as sco\n",
        "from functools import reduce\n",
        "import time\n",
        "import gc\n",
        "from pytrends.request import TrendReq\n",
        "from fredapi import Fred\n",
        "import quandl\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "api = '86c3322708a8cb34c8f59b85318f8da8'\n",
        "quandl.ApiConfig.api_key = 'ezG_brK9DueLj9Xsdv1r'\n",
        "fred = Fred(api_key=api)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ratelimit\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/38/ff60c8fc9e002d50d48822cc5095deb8ebbc5f91a6b8fdd9731c87a147c9/ratelimit-2.2.1.tar.gz\n",
            "Building wheels for collected packages: ratelimit\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-cp37-none-any.whl size=5908 sha256=d898c52bc6517ed408a17d640a6a2ee84bf61f4a4f9aa4c3bb3b35e57bbf2dd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/d9/82/3c6044cf1a54aab9151612458446d9b17a38416869e1b1d9b8\n",
            "Successfully built ratelimit\n",
            "Installing collected packages: ratelimit\n",
            "Successfully installed ratelimit-2.2.1\n",
            "Collecting pytrends\n",
            "  Downloading https://files.pythonhosted.org/packages/96/53/a4a74c33bfdbe1740183e00769377352072e64182913562daf9f5e4f1938/pytrends-4.7.3-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytrends) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from pytrends) (1.1.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pytrends) (4.2.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.7.3\n",
            "Collecting fredapi\n",
            "  Downloading https://files.pythonhosted.org/packages/db/82/9ca1e4a7f1b2ae057e8352cc46d016866c067a12d013fc05af0670f4291a/fredapi-0.4.3-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fredapi) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fredapi) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->fredapi) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fredapi) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->fredapi) (1.15.0)\n",
            "Installing collected packages: fredapi\n",
            "Successfully installed fredapi-0.4.3\n",
            "Collecting quandl\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/2b/feefb36015beaecc5c0f9f2533e815b409621d9fa7b50b2aac621796f828/Quandl-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from quandl) (8.8.0)\n",
            "Collecting inflection>=0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/59/91/aa6bde563e0085a02a435aa99b49ef75b0a4b062635e606dab23ce18d720/inflection-0.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from quandl) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from quandl) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from quandl) (2.8.1)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Installing collected packages: inflection, quandl\n",
            "Successfully installed inflection-0.5.1 quandl-3.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrROc4MHMZlN"
      },
      "source": [
        "DIRECTORY = '/content/drive/My Drive/Trading/' # сюда написать рабочую директорию куда сохранять все файлы\n",
        "START_DOWNLOAD = '1990-01-01'\n",
        "END_DOWNLOAD = '2021-06-01'\n",
        "API_FINANCIALMODELINGPREP = '37d9e9f7c27eb5b3bf7a63e606b030ed'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROd0NIc1OpmS"
      },
      "source": [
        "### Скачиваем котировки, pl, bs, key metrics, financial growth по всем тикерам, торгующимся на NASDAQ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmv5bdkMMAlF"
      },
      "source": [
        "ONE_MINUTE = 60 \n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=300, period=ONE_MINUTE)\n",
        "def get_jsonparsed_data(url):\n",
        "    \"\"\"\n",
        "    Receive the content of ``url``, parse it as JSON and return the object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "    \"\"\"\n",
        "    response = urlopen(url)\n",
        "    data = response.read().decode(\"utf-8\")\n",
        "    return json.loads(data)\n",
        "\n",
        "def reduce_mem_usage(df, verbose=False):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "\n",
        "## Simple \"Memory profilers\" to see memory usage\n",
        "def get_memory_usage():\n",
        "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
        "        \n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "\n",
        "# print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(data.memory_usage(index=True).sum())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-sHmZUrMDSq"
      },
      "source": [
        "# загружаем список всех доступных тикеров\n",
        "\n",
        "url = \"https://financialmodelingprep.com/api/v3/stock/list?apikey={}\".format(API_FINANCIALMODELINGPREP)\n",
        "symbols = pd.DataFrame(get_jsonparsed_data(url))\n",
        "\n",
        "# отбираем тикеры для скачки только те, что торгуются на насдаке\n",
        "stock_exchange_list = [ 'Nasdaq Global Select',\n",
        "                       'Nasdaq Global Market',\n",
        "                       'Nasdaq Capital Market', 'NASDAQ', 'Nasdaq', \n",
        "                       'NasdaqGS',\n",
        "                       'NasdaqGM'\n",
        "                        ]\n",
        "\n",
        "symbols_to_download = symbols[symbols['exchange'].isin(stock_exchange_list)]['symbol'].sort_values().values   \n",
        "len(symbols_to_download)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJEgcMgLMKVN"
      },
      "source": [
        "# скачиваем и схохраняем исторические цены по выбранным тикерам за указанный период\n",
        "\n",
        "url = \"https://financialmodelingprep.com/api/v3/historical-price-full/{}?from={}&to=&apikey={}\".format(START_DOWNLOAD, END_DOWNLOAD, API_FINANCIALMODELINGPREP) \n",
        "\n",
        "historical_price_full = []\n",
        "skipped_symbols = []\n",
        "for symbol in symbols_to_download:\n",
        "  try:\n",
        "    tmp = pd.json_normalize(get_jsonparsed_data(url.format(symbol))['historical'])\n",
        "  except:\n",
        "    skipped_symbols.append(symbol)\n",
        "    pass\n",
        "    \n",
        "  if not tmp.empty:\n",
        "    tmp['symbol'] = symbol\n",
        "    historical_price_full.append(reduce_mem_usage(tmp))\n",
        "\n",
        "df_full = pd.concat(historical_price_full)\n",
        "\n",
        "with open('{}historical_price_last_year.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(df_full, f) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY7VAb1MOik"
      },
      "source": [
        "# Определяем ссылки для прочей информации\n",
        "\n",
        "links = {}\n",
        "\n",
        "links['pl'] = \"https://financialmodelingprep.com/api/v3/income-statement/{}?from={}&to={}&apikey={}\" # PL\n",
        "links['bs'] = \"https://financialmodelingprep.com/api/v3/balance-sheet-statement/{}?from={}}&to={}}&apikey={}\" # BS\n",
        "links['financial_growth'] = \"https://financialmodelingprep.com/api/v3/financial-growth/{}?from={}}&to={}}&apikey={}\" # financial_growth\n",
        "links['key_metrics'] = \"https://financialmodelingprep.com/api/v3/key-metrics/{}?from={}&to={}&apikey={}\" # key_metrics\n",
        "\n",
        "\n",
        "# скачиваем всю прочую информацию\n",
        "\n",
        "df_dict = {}\n",
        "\n",
        "for link in ['pl', 'bs', 'financial_growth', 'key_metrics']:\n",
        "  df = []\n",
        "  for symbol in symbols_to_download:\n",
        "    url = links[link].format(symbol, START_DOWNLOAD, END_DOWNLOAD, API_FINANCIALMODELINGPREP)\n",
        "    df.append(pd.DataFrame(get_jsonparsed_data(url)))\n",
        "\n",
        "  with open('{}{}_last_year.pickle'.format(DIRECTORY,link), 'wb') as f:\n",
        "    pickle.dump(pd.concat(df), f) \n",
        "\n",
        "  print(link, 'OK')\n",
        "\n",
        "  df_dict[link] = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52IF1HpPs4p"
      },
      "source": [
        "### Обогащаем датасет с помощью feature - engineering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWowlODvPybr"
      },
      "source": [
        "def merge_with_calendar(quotes):\n",
        "  # Поскольку могут быть торговые дни без сделок, данных по таким дням нет, однако, объем сделок = 0 нужен, чтобы корректно считать скользящую по объему!\n",
        "  # В качестве календаря для мерджа берется список дней в которых торговался эпл (предполагаем что в каждом торговом дне у эпла были сделки)\n",
        "  dates = pd.DataFrame(quotes[quotes['symbol']=='AAPL']['date'])\n",
        "  quotes_df = pd.DataFrame([])\n",
        "\n",
        "  for ticker in quotes['symbol'].unique():\n",
        "\n",
        "    if quotes_df.empty:\n",
        "\n",
        "      quotes_df = dates.merge(quotes[quotes['symbol']==ticker], how='left', on='date')\n",
        "      quotes_df['symbol'] = quotes_df['symbol'].fillna(method='bfill')\n",
        "      quotes_df.dropna(axis=0, subset=['symbol'], inplace=True)\n",
        "    \n",
        "    else:\n",
        "\n",
        "      tmp = dates.merge(quotes[quotes['symbol']==ticker], how='left', on='date')\n",
        "      tmp['symbol'] = tmp['symbol'].fillna(method='bfill')\n",
        "      tmp.dropna(axis=0, subset=['symbol'], inplace=True)\n",
        "      quotes_df = quotes_df.append(tmp)\n",
        "\n",
        "  return quotes_df.drop_duplicates()\n",
        "\n",
        "def fill_na_days(quotes_df):\n",
        "  # Поскольку могут быть торговые дни без сделок, данных по таким дням нет, однако, объем сделок = 0 нужен, чтобы корректно считать скользящую по объему!\n",
        "  quotes_df[['volume', 'unadjustedVolume', 'change', 'changePercent', 'changeOverTime']] = quotes_df[['volume', 'unadjustedVolume', 'change', 'changePercent', 'changeOverTime']].fillna(0)\n",
        "  quotes_df[['open', 'high', 'low', 'close', 'adjClose', 'vwap']] = quotes_df[['open', 'high', 'low', 'close', 'adjClose', 'vwap']].fillna(method='bfill')\n",
        "\n",
        "  return quotes_df\n",
        "\n",
        "def total_return_from_returns(returns):\n",
        "  # считает месячное изменение на основе дневных изменений в %\n",
        "  return (returns + 1).prod() - 1\n",
        "\n",
        "def target_category(pct_change):\n",
        "  # делаем таргет\n",
        "  if pct_change >= 0.1:\n",
        "    return 0\n",
        "  if pct_change >= 0.05 and pct_change < 0.1:\n",
        "    return 1\n",
        "  if pct_change >= 0 and pct_change < 0.05: \n",
        "    return 2\n",
        "  if pct_change >= -0.1 and pct_change < 0:\n",
        "    return 3\n",
        "  if pct_change < -0.1:\n",
        "    return 4\n",
        "\n",
        "def calc_pct_change(data):\n",
        "  return (data.iloc[-1] - data.iloc[0]) / data.iloc[0]\n",
        "\n",
        "def next_month_return_and_target_alt(quotes_df):\n",
        "\n",
        "  quotes_df['date'] = pd.to_datetime(quotes_df['date'])\n",
        "  quotes_df['year'] = quotes_df['date'].dt.year\n",
        "  quotes_df['month'] = quotes_df['date'].dt.month\n",
        "\n",
        "  # считаем изменение за текущий календарный месяц и делаем сдвиг назад на 1 месяц\n",
        "  res = quotes_df[['symbol', 'year', 'month', 'close']].groupby(['symbol', 'year', 'month']).apply(calc_pct_change).reset_index()\n",
        "  res['close'] = res[['symbol', 'close']].groupby('symbol').shift(-1)\n",
        "  res.columns = ['symbol', 'year', 'month', 'price_change_next_calendar_month']\n",
        "\n",
        "  # мерджим результат с основным дф\n",
        "  quotes_df = quotes_df.merge(res, how='left', on=['symbol', 'year', 'month'])\n",
        "\n",
        "  # считаем категорию для таргета\n",
        "  quotes_df['target'] = quotes_df['price_change_next_calendar_month'].apply(target_category)\n",
        "\n",
        "  return quotes_df\n",
        "\n",
        "\n",
        "def next_month_return_and_target(quotes_df, col='daily_pct_change'):\n",
        "  # считаем изменение за следующий торговый месяц в %\n",
        "  quotes_df['date'] = pd.to_datetime(quotes_df['date'])\n",
        "  quotes_df['year'] = quotes_df['date'].dt.year\n",
        "  quotes_df['month'] = quotes_df['date'].dt.month\n",
        "\n",
        "  # считаем изменение в % за текущий торговый месяц\n",
        "  month_changes = quotes_df.reset_index().set_index(\"date\")\n",
        "  month_changes = month_changes.groupby(['symbol', 'year', 'month'])[[col]].apply(total_return_from_returns)\n",
        "  month_changes = month_changes.reset_index()\n",
        "\n",
        "  # Изменение цены в % за следующий торговый месяц\n",
        "  month_changes['price_change_next_calendar_month'] = month_changes.groupby('symbol')[col].shift(-1)\n",
        "\n",
        "  # считаем категорию для таргета\n",
        "  month_changes['target'] = month_changes['price_change_next_calendar_month'].apply(target_category)\n",
        "\n",
        "  quotes_df = quotes_df.merge(month_changes[['symbol', 'year', 'month', 'price_change_next_calendar_month', 'target']], on=['symbol', 'year', 'month'], how='left')\n",
        "\n",
        "  return quotes_df.dropna(axis=0, subset=['target'])\n",
        "\n",
        "def business_days(quotes_df):\n",
        "  offset = BMonthEnd()\n",
        "  #Last day of current month\n",
        "  quotes_df['last_bday_current_month'] = quotes_df['date'].apply(lambda x: offset.rollforward(x))\n",
        "  quotes_df['is_last_bday_current_month'] = (quotes_df['last_bday_current_month'] == quotes_df['date'])\n",
        "  # Last day of previous month\n",
        "  # quotes_df['last_bday_previous_month'] = quotes_df['date'].apply(lambda x: offset.rollback(x))\n",
        "\n",
        "  return quotes_df\n",
        "\n",
        "def daily_returns(quotes_df):\n",
        "  return quotes.groupby('symbol')['close'].pct_change()\n",
        "\n",
        "def wma_calc(data, period):\n",
        "  return data.ewm(span=period, adjust=False).mean()\n",
        "\n",
        "def VOSC(quotes_df, short_period=12, long_period=26):\n",
        "  VEMA12 = quotes_df[['symbol', 'volume']].groupby('symbol').apply(lambda x: wma_calc(x, short_period)).reset_index(drop=True) #\n",
        "  VEMA26 = quotes_df[['symbol', 'volume']].groupby('symbol').apply(lambda x: wma_calc(x, long_period)).reset_index(drop=True) #\n",
        "  VOSC = (VEMA26['volume'] - VEMA12['volume']) / VEMA12['volume'] * 100\n",
        "  return VOSC\n",
        "\n",
        "def VMACD2(quotes_df, short_period=12, long_period=26, signal_period=9):\n",
        "  quotes_df['VEMA12'] = quotes_df[['symbol', 'volume']].groupby('symbol').apply(lambda x: wma_calc(x, short_period)).reset_index(drop=True)['volume']\n",
        "  quotes_df['VEMA26'] = quotes_df[['symbol', 'volume']].groupby('symbol').apply(lambda x: wma_calc(x, long_period)).reset_index(drop=True)['volume']\n",
        "  quotes_df['MACD'] = (quotes_df['VEMA12'] - quotes_df['VEMA26']) / quotes_df['volume'] * 100\n",
        "  quotes_df['signal'] = quotes_df[['symbol', 'MACD']].groupby('symbol').apply(lambda x: wma_calc(x, signal_period)).reset_index(drop=True)['MACD']\n",
        "  quotes_df['signal'] = quotes_df['signal'] / quotes_df['volume'] * 100\n",
        "  quotes_df['MACD_diff'] = quotes_df['MACD'] - quotes_df['signal']\n",
        "  return quotes_df['MACD_diff']\n",
        "\n",
        "def calc_pct_change(data):\n",
        "  return (data.iloc[-1] - data.iloc[0]) / data.iloc[0]\n",
        "\n",
        "def ROC(quotes_df, period=20, column='close'):\n",
        "  return quotes_df[['symbol', column]].groupby('symbol').rolling(period, min_periods=2).apply(calc_pct_change).reset_index()[column]\n",
        "\n",
        "def VOLUME(quotes_df, period=20):\n",
        "  # Объем торгов за день / Объем торгов за последние 20 дней MEAN * Скорость возврата за последние 20 дней MEAN\n",
        "  mean_volume_20 = quotes_df[['symbol', 'volume']].groupby('symbol').rolling(period, min_periods=2).mean().reset_index()['volume']\n",
        "\n",
        "  return quotes_df['volume'] / mean_volume_20 * quotes_df['ROC']\n",
        "\n",
        "def Price1M(quotes_df):\n",
        "\n",
        "  mean_close_21 = quotes_df[['symbol', 'close']].groupby('symbol').rolling(21, min_periods=2).mean().reset_index()['close']\n",
        "\n",
        "  return quotes_df['close'] / mean_close_21 - 1\n",
        "\n",
        "def MAC(quotes_df, period=20):\n",
        "\n",
        "  MA20 = quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=2).mean().reset_index()['close']\n",
        "\n",
        "  return MA20 / quotes_df['close']\n",
        "\n",
        "def VARIANCE(quotes_df, period=20, column='daily_pct_change'):\n",
        "  return quotes_df[['symbol', column]].groupby('symbol').rolling(period, min_periods=2).apply(lambda x: np.var(x)).reset_index()[column]\n",
        "\n",
        "def KURTOSIS(quotes_df, period=20):\n",
        "  return quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=2).apply(lambda x: kurtosis(x)).reset_index()['close']\n",
        "\n",
        "def SKEW(quotes_df, period=20):\n",
        "  return quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=2).apply(lambda x: skew(x)).reset_index()['close']\n",
        "\n",
        "def TRIX(quotes_df, period=10):\n",
        "  quotes_df['EMA1'] = quotes_df[['symbol', 'close']].groupby('symbol').apply(lambda x: wma_calc(x, period=10)).reset_index(drop=True)['close']\n",
        "  quotes_df['EMA2'] = quotes_df[['symbol', 'EMA1']].groupby('symbol').apply(lambda x: wma_calc(x, period=10)).reset_index(drop=True)['EMA1']\n",
        "  quotes_df['EMA3'] = quotes_df[['symbol', 'EMA2']].groupby('symbol').apply(lambda x: wma_calc(x, period=10)).reset_index(drop=True)['EMA2']\n",
        "  TRIX10 = quotes_df[['symbol', 'EMA3']].groupby('symbol').pct_change()\n",
        "  return TRIX10\n",
        "\n",
        "def money_ratio(data):\n",
        "  positive_money_flow = sum(data[data>0])\n",
        "  negative_money_flow = sum(data[data<0])\n",
        "\n",
        "  if negative_money_flow == 0:\n",
        "    negative_money_flow = 1\n",
        "\n",
        "  money_ratio = - positive_money_flow / negative_money_flow\n",
        "\n",
        "  return 100 - (100 / (1 + money_ratio))\n",
        "\n",
        "def MFI(quotes_df, period=14):\n",
        "  typical = (quotes_df['high'] + quotes_df['low'] + quotes_df['close']) / 3\n",
        "  money_flow = pd.DataFrame(typical * quotes_df['volume'])\n",
        "  money_flow['symbol'] = quotes_df['symbol']\n",
        "  money_flow['pct_change'] = money_flow.groupby('symbol').pct_change().fillna(0)\n",
        "  money_flow.loc[money_flow['pct_change'] >= 0, 'sign'] = 1\n",
        "  money_flow.loc[money_flow['pct_change'] < 0, 'sign'] = -1\n",
        "  money_flow['money_flow'] = money_flow[0] * money_flow['sign']\n",
        "  return money_flow[['symbol', 'money_flow']].groupby('symbol').rolling(period, min_periods=2).apply(money_ratio).reset_index()['money_flow']\n",
        "\n",
        "def boll_up(quotes_df, D=2, period=20):\n",
        "  SMA = quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=2).mean().reset_index()[['symbol', 'close']]\n",
        "  STD = quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=2).std().reset_index()[['symbol', 'close']]\n",
        "  return (SMA['close'] + D*STD['close']) / quotes_df['close']\n",
        "\n",
        "def boll_down(quotes_df, D=2, period=20):\n",
        "  SMA = quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=2).mean().reset_index()[['symbol', 'close']]\n",
        "  STD = quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=2).std().reset_index()[['symbol', 'close']]\n",
        "  return (SMA['close'] - D*STD['close']) / quotes_df['close']\n",
        "\n",
        "def CAGR(data):\n",
        "  data = list(data)\n",
        "  days_to_year = len(data)/251\n",
        "  return (data[-1]/data[0])**(1/days_to_year) - 1\n",
        "\n",
        "def SHARPE_RATIO(quotes_df, period=20, risk_free_rate=0.04):\n",
        "  VOL = quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=period).std().reset_index()[['symbol', 'close']]\n",
        "  RP = quotes_df[['symbol', 'close']].groupby('symbol').rolling(period, min_periods=period).apply(CAGR).reset_index()[['symbol', 'close']] # считаем среднее дневное изменение за период\n",
        "  RP['close'] = RP['close'] - risk_free_rate # числитель коэффициента шарпа\n",
        "  return RP['close'] / VOL['close'] * math.sqrt(252) # 252 торговых дня в году = годовая доходность\n",
        "\n",
        "def load_fundamentals(quotes_df):\n",
        "  with open('{}financial_growth_last_year.pickle'.format(DIRECTORY), 'rb') as f: #fin_growth\n",
        "    fin_growth = pickle.load(f)\n",
        "  with open('{}key_metrics_last_year.pickle'.format(DIRECTORY), 'rb') as f: #key_metrics\n",
        "    key_metrics = pickle.load(f)\n",
        "  with open('{}pl_last_year.pickle'.format(DIRECTORY), 'rb') as f: #financial-statement_annual\n",
        "    PL = pickle.load(f)\n",
        "  with open('{}bs_last_year.pickle'.format(DIRECTORY), 'rb') as f: #balance-sheet_annual\n",
        "    balance_sheet_annual = pickle.load(f)\n",
        "\n",
        "\n",
        "  fundamentals_data = PL[['date', 'symbol', 'period','fillingDate', 'acceptedDate',\n",
        "                          'ebitdaratio', 'operatingIncomeRatio', \n",
        "                          'incomeBeforeTaxRatio', 'netIncomeRatio', 'eps', \n",
        "                          'weightedAverageShsOut',\n",
        "                          'ebitda', 'incomeBeforeTax', 'interestExpense', 'revenue']]\n",
        "                          \n",
        "  fundamentals_data = fundamentals_data.merge(fin_growth, \n",
        "                                              on=['date', 'symbol', 'period'], \n",
        "                                              how='left') \n",
        "   \n",
        "  fundamentals_data = fundamentals_data.merge(key_metrics, \n",
        "                                              on=['date', 'symbol', 'period'], \n",
        "                                              how='left')\n",
        "  \n",
        "  fundamentals_data = fundamentals_data.merge(balance_sheet_annual[['symbol','fillingDate',\n",
        "                                                                    'totalStockholdersEquity', \n",
        "                                                                    'retainedEarnings', \n",
        "                                                                    'longTermDebt']], \n",
        "                                              on=['fillingDate', 'symbol'], \n",
        "                                              how='left')\n",
        "\n",
        "  fundamentals_data.rename(columns={'date': 'period_end'}, inplace=True)\n",
        "  fundamentals_data['date'] = pd.to_datetime(fundamentals_data['acceptedDate'])\n",
        "\n",
        "  return quotes_df.merge(fundamentals_data, how='left', \n",
        "                         on=['date', 'symbol'])\n",
        "\n",
        "def load_profile(quotes_df):\n",
        "  with open('{}profile_full.pickle'.format(DIRECTORY), 'rb') as f: \n",
        "    profile = pickle.load(f)\n",
        "\n",
        "  profile['exchangeShortName'] = profile['exchangeShortName'].astype('category')\n",
        "  profile['industry'] = profile['industry'].astype('category')\n",
        "  profile['ceo'] = profile['ceo'].astype('category')\n",
        "  profile['sector'] = profile['sector'].astype('category')\n",
        "  profile['country'] = profile['country'].astype('category')\n",
        "  profile['fullTimeEmployees'] = profile['fullTimeEmployees'].fillna(0).replace({'': 0}).astype(int)\n",
        "  \n",
        "  return quotes_df.merge(profile[['symbol', 'exchangeShortName', 'industry',\n",
        "                           'ceo', 'sector', 'country', 'fullTimeEmployees',\n",
        "                           'isActivelyTrading', 'isEtf']], on='symbol', how='left')\n",
        "\n",
        "def p_b_ratio_daily(quotes_df):\n",
        "  book = quotes_df['totalStockholdersEquity'] / quotes_df['weightedAverageShsOut']\n",
        "  price = quotes_df['close']\n",
        "  return book / price\n",
        "\n",
        "def p_b_ratio_daily_alter(quotes_df):\n",
        "  return quotes_df['bookValuePerShare'] / quotes_df['close']\n",
        "\n",
        "def net_profit_to_total_operate_revenue_ttm(quotes_df):\n",
        "  return quotes_df['netIncomePerShare'] / quotes_df['revenuePerShare']\n",
        "\n",
        "def market_cap_daily(quotes_df):\n",
        "  return quotes_df['weightedAverageShsOut'] * quotes_df['close']\n",
        "\n",
        "def VOL(quotes_df, period=20):\n",
        "  # Среднее значение 20-дневной оборачиваемости, в% (объем сделок поделить на всего капитализацию)\n",
        "  quotes_df['vol_daily'] = quotes_df['volume'] / quotes_df['weightedAverageShsOut'] * quotes_df['vwap']\n",
        "  mean_vol = quotes_df[['symbol', 'vol_daily']].groupby('symbol').rolling(period, min_periods=2).mean().reset_index()[['symbol', 'vol_daily']]\n",
        "  return mean_vol['vol_daily']\n",
        "\n",
        "def DAVOL(quotes_df, period_short=20, period_long=120):\n",
        "  return VOL(quotes_df, period=20) / VOL(quotes_df, period=120)\n",
        "\n",
        "def retained_earnings_per_share(quotes_df):\n",
        "  return quotes_df['retainedEarnings'] / quotes_df['weightedAverageShsOut']\n",
        "\n",
        "def cash_flow_to_price_ratio(quotes_df):\n",
        "  return quotes_df['freeCashFlowPerShare'] / quotes_df['close']\n",
        "\n",
        "def financial_liability_to_price(quotes_df):\n",
        "  # financial_liability\n",
        "  return quotes_df['longTermDebt'] / quotes_df['weightedAverageShsOut'] / quotes_df['close']\n",
        "\n",
        "def ACCA(quotes_df):\n",
        "  return quotes_df['freeCashFlowPerShare'] / quotes_df['p_b_ratio_daily_alter']\n",
        "\n",
        "def sales_to_price_daily(quotes_df):\n",
        "  book = quotes_df['revenue'] / quotes_df['weightedAverageShsOut']\n",
        "  price = quotes_df['close']\n",
        "  return book / price\n",
        "\n",
        "def earnings_to_price_ratio(quotes_df):\n",
        "  book = quotes_df['incomeBeforeTax'] / quotes_df['weightedAverageShsOut']\n",
        "  price = quotes_df['close']\n",
        "  return book / price\n",
        "\n",
        "def calculate_rank(quotes_df, column_to_rank, group_by_columns, ascending=False, method='max'):\n",
        "  return quotes_df.groupby(group_by_columns)[column_to_rank].rank(ascending=ascending, method=method)\n",
        "\n",
        "\n",
        "def google_trends(quotes_df):\n",
        "  # скачивает с гугл трендс историю поиска по ключевым словам заданным ниже\n",
        "  pytrends = TrendReq(hl='en-US', tz=360, timeout=(9,9))\n",
        "\n",
        "  kw_list = [\"debt\", 'color', 'stocks', 'restaurant', 'portfolio', 'inflation', 'housing', 'ring',\n",
        "            'crisis financial', 'stock crash','labour', 'stock market','buy and hold', 'unemployment',\n",
        "            ]\n",
        "\n",
        "#              'economics', 'credit', 'markets', 'return',  'money', 'religion', 'cancer', 'growth',\n",
        "#             'investment', 'hedge', 'marriage', 'bonds', 'derivatives', 'headlines', 'profit', 'society', \n",
        "#              'leverage', 'loss', 'cash', 'office', 'fine',  'banking', 'crisis', 'happy', 'car',\n",
        "#             'nasdaq', 'gains', 'finance', 'sell', 'invest', 'fed', 'house', 'metals', 'travel', 'returns', 'gain', 'default', 'present', 'holiday', 'water', 'rich', 'risk', 'gold', 'success', 'oil', 'war',\n",
        "#             'economy', 'dow jones', 'chance', 'short sell', 'lifestyle', 'greed', 'food', 'financial market', 'movie', 'nyse', 'ore', 'opporunity', 'health', 'short selling', 'earnings', 'arts',\n",
        "#             'culture', 'bubble', 'buy', 'trader', 'rare earths', 'tourism', 'politics', 'energy', 'consume', 'consumtion', 'freedom', 'dividend', 'world', 'conflict', 'kitchen','forex', 'home', 'crash', 'transaction',\n",
        "#             'garden', 'fond', 'train', 'fun', 'environment',  ]\n",
        "\n",
        "  google_trends_data = []\n",
        "\n",
        "  def request_data(request_d):\n",
        "    try:\n",
        "      pytrends.build_payload([request_d], cat=0, timeframe='all', geo='US', gprop='') \n",
        "      google_trends_data.append(pytrends.interest_over_time())\n",
        "    except:\n",
        "      time.sleep(1)\n",
        "      request_data(request_d)\n",
        "\n",
        "  for request in kw_list:\n",
        "    try:\n",
        "      request_data(request)\n",
        "    except:\n",
        "      print('skip request {}'.format(request))\n",
        "      pass\n",
        "\n",
        "  trends_df = reduce(lambda x, y: pd.merge(x, y, on = 'date'), google_trends_data)\n",
        "  try:\n",
        "    trends_df = trends_df.drop(['isPartial'], axis=1)\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    trends_df = trends_df.drop(['isPartial_x', 'isPartial_y'], axis=1)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  trends_df = trends_df.pct_change(periods=3)\n",
        "  cols = trends_df.columns\n",
        "  trends_df = trends_df.reset_index() # считаем изменение за месяц по гугл трендс!\n",
        "  dates = pd.DataFrame(pd.date_range(start='1/1/2004', end='20/05/2021'))\n",
        "  dates.columns = ['date']\n",
        "  dates = dates.merge(trends_df, how='left', on='date').fillna(method='ffill')\n",
        "  return cols, quotes_df.merge(dates, on='date', how='left')\n",
        "\n",
        "\n",
        "def fill_shares_number(quotes_df):\n",
        "  quotes_df.loc[quotes_df['weightedAverageShsOut']==0, 'weightedAverageShsOut'] = round(quotes_df['revenue'] / quotes_df['revenuePerShare'],0)\n",
        "  return quotes_df\n",
        "\n",
        "\n",
        "def set_ff_groups_median(quotes_df):\n",
        "  # устанавливает группы акций по 3 факторной модели Ferma-French \n",
        "  # на основе этих групп можно будет расчитать историческое изменение цен по этим группам\n",
        "  market_cap_median = quotes_df['market_cap_daily'].median()\n",
        "  bp_median = quotes_df['bookValuePerShare'].median()\n",
        "  profit_median = quotes_df['LAST_YEAR_PERFORMANCE'].median()\n",
        "\n",
        "  mask_cap_small = quotes_df['market_cap_daily'] < market_cap_median\n",
        "  mask_cap_big = quotes_df['market_cap_daily'] >= market_cap_median\n",
        "  mask_book_low = quotes_df['bookValuePerShare'] < bp_median\n",
        "  mask_book_high = quotes_df['bookValuePerShare'] >= bp_median\n",
        "  mask_profit_high = quotes_df['LAST_YEAR_PERFORMANCE'] >= profit_median\n",
        "  mask_profit_low = quotes_df['LAST_YEAR_PERFORMANCE'] < profit_median\n",
        "\n",
        "  # Book & Cap size\n",
        "  quotes_df['ff_groups_book_median'] = None\n",
        "  quotes_df['ff_groups_book_median'].loc[mask_cap_small&mask_book_high] = 'sh'\n",
        "  quotes_df['ff_groups_book_median'].loc[mask_cap_big&mask_book_high] = 'bh'\n",
        "  quotes_df['ff_groups_book_median'].loc[mask_cap_small&mask_book_low] = 'sl'\n",
        "  quotes_df['ff_groups_book_median'].loc[mask_cap_big&mask_book_low] = 'bl'\n",
        "  quotes_df['ff_groups_book_median'] = quotes_df['ff_groups_book_median'].astype('category')\n",
        "\n",
        "  # Profit & Cap size\n",
        "  quotes_df['ff_groups_profit_median'] = None\n",
        "  quotes_df['ff_groups_profit_median'].loc[mask_cap_small&mask_profit_high] = 'sr'\n",
        "  quotes_df['ff_groups_profit_median'].loc[mask_cap_big&mask_profit_high] = 'br'\n",
        "  quotes_df['ff_groups_profit_median'].loc[mask_cap_small&mask_profit_low] = 'sw'\n",
        "  quotes_df['ff_groups_profit_median'].loc[mask_cap_big&mask_profit_low] = 'bw'\n",
        "  quotes_df['ff_groups_profit_median'] = quotes_df['ff_groups_profit_median'].astype('category')\n",
        "  \n",
        "  return quotes_df[['ff_groups_book_median', 'ff_groups_profit_median']]\n",
        "  # return quotes_df\n",
        "\n",
        "\n",
        "\n",
        "def calculate_ff_group_returns(quotes_df):\n",
        "  quotes_df[['market_cap_daily', 'daily_pct_change']] = quotes_df[['market_cap_daily', 'daily_pct_change']].fillna(0)\n",
        "  quotes_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "  ff_columns = ['ff_groups_book_median', 'ff_groups_profit_median']\n",
        "  periods = [251, 60, 20]\n",
        "\n",
        "  for ff_column in ff_columns:\n",
        "\n",
        "    wm = lambda x: np.average(x, weights=quotes_df.loc[x.index, \"market_cap_daily\"])\n",
        "    tmp = quotes_df[['date', ff_column, 'market_cap_daily', 'daily_pct_change']].dropna().groupby(['date', ff_column]).agg(total_cap_ff_group=(\"market_cap_daily\", \"sum\"),  \n",
        "                                                        daily_pct_change_weighted_by_cap_mean=(\"daily_pct_change\", wm))\n",
        "\n",
        "    tmp = tmp.reset_index()\n",
        "    tmp.rename(columns={'total_cap_ff_group': 'total_cap_{}'.format(ff_column),\n",
        "                        'daily_pct_change_weighted_by_cap_mean': 'daily_pct_change_weighted_by_cap_{}'.format(ff_column)}, inplace=True) \n",
        "    \n",
        "    tmp.set_index('date', drop=True, inplace=True)\n",
        "    quotes_df = quotes_df.merge(tmp, how='left', on=['date', ff_column])\n",
        "    \n",
        "    for period in periods:\n",
        "\n",
        "      group_returns = tmp[[ff_column, 'daily_pct_change_weighted_by_cap_{}'.format(ff_column)]].groupby(ff_column).rolling(period, min_periods=2).apply(total_return_from_returns)\n",
        "      group_returns = group_returns.reset_index(level=0)\n",
        "      group_returns.rename(columns={'daily_pct_change_weighted_by_cap_{}'.format(ff_column): 'pct_change_{}_{}'.format(ff_column, period)}, inplace=True)\n",
        "\n",
        "      quotes_df = quotes_df.merge(group_returns, how='left', on=['date', ff_column])\n",
        "\n",
        "      tmp2 = group_returns.pivot_table(columns=ff_column, values='pct_change_{}_{}'.format(ff_column, period), index='date')\n",
        "      tmp2.columns = tmp2.columns.astype(str)\n",
        "      tmp2 = tmp2.reset_index()\n",
        "\n",
        "      if period == 251: # эти параметры считаем только для годовых значений\n",
        "\n",
        "        if ff_column == 'ff_groups_book_median':\n",
        "          tmp2['HML'] = (tmp2['bh'] + tmp2['sh'])/2 - (tmp2['sl'] + tmp2['bl'])/2\n",
        "          tmp2['SMB'] = (tmp2['sl'] + tmp2['sh'])/2  - (tmp2['bl'] + tmp2['bh'])/2 \n",
        "          cols_to_merge = ['date', 'bh', 'bl', 'sh', 'sl', 'HML', 'SMB']\n",
        "          quotes_df = quotes_df.merge(tmp2[cols_to_merge], how='left', on='date')\n",
        "        \n",
        "        elif ff_column == 'ff_groups_profit_median':\n",
        "          tmp2['RMW'] = (tmp2['br'] + tmp2['sr'])/2 - (tmp2['sw'] + tmp2['bw'])/2\n",
        "          cols_to_merge = ['date', 'sr', 'br', 'sw', 'bw', 'RMW']\n",
        "          quotes_df = quotes_df.merge(tmp2[cols_to_merge], how='left', on='date')\n",
        "\n",
        "  return quotes_df\n",
        "\n",
        "def load_quandl(quotes_df):\n",
        "\n",
        "    dailySeries = ['LBMA/GOLD']\n",
        "\n",
        "    for timeSeries in dailySeries:\n",
        "        try:\n",
        "            df = quandl.get(timeSeries, column_index='1')\n",
        "            df.columns = [timeSeries]\n",
        "            df[timeSeries+'PctDaily'] = df[timeSeries].pct_change()\n",
        "            df[timeSeries+'60dChange'] = df[timeSeries+'PctDaily'].rolling(60, min_periods=2).apply(total_return_from_returns)\n",
        "            df[timeSeries+'20dChange'] = df[timeSeries+'PctDaily'].rolling(20, min_periods=2).apply(total_return_from_returns)\n",
        "            df = df.reset_index().rename(columns={'Date': 'date'})\n",
        "            df = df.drop(timeSeries+'PctDaily', axis=1)\n",
        "            quotes_df = quotes_df.merge(df, on='date', how='left')\n",
        "            \n",
        "        except:\n",
        "            pass\n",
        "            \n",
        "    return quotes_df\n",
        "    \n",
        "\n",
        "def load_FRED_data(quotes_df):\n",
        "  # T10YFF = 10-Year Treasury Constant Maturity Minus Federal Funds Rate = дни \\ на большом датасете можно будет оставить как есть, добавить изменения\n",
        "  # T10Y2Y = 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity = дни \\ на большом датасете можно будет оставить как есть, добавить изменения\n",
        "  # BAMLCC7A01015YTRIV = ICE BofA 10-15 Year US Corporate Index Total Return Index Value = индекс облигаций, дни, надо к изменениям переходить\n",
        "  # FEDFUNDS = FED Funds Rate = к изменениям переходить \\ месяц\n",
        "  # DBAA = Moody's Seasoned Baa Corporate Bond Yield = день, перейти к изменениям\n",
        "  # NASDAQCOM = NASDAQ / дни\n",
        "  # SP500 / дни\n",
        "  # PMETAINDEXM -  Global price of Metal index / month\n",
        "  # DCOILWTICO - Crude Oil Prices: West Texas Intermediate (WTI) - Cushing, Oklahoma / day\n",
        "  # DCOILBRENTEU Crude Oil Prices: Brent - Europe/day\n",
        "  # GOLDAMGBD228NLBM - Gold Fixing Price 10:30 A.M. (London time) in London Bullion Market, based in U.S. Dollars /day\n",
        "  # DHHNGSP - Henry Hub Natural Gas Spot Price\n",
        "  # DJFUELUSGULF - Kerosene-Type Jet Fuel Prices: U.S. Gulf Coast\n",
        "  # DGASUSGULF - Conventional Gasoline Prices: U.S. Gulf Coast, Regular\n",
        "  # PCOPPUSDM - copper / mont\n",
        "  # PALLFNFINDEXM - Global Price Index of All Commodities /month\n",
        "  # PIORECRUSDM - iron ore /month\n",
        "  # PWHEAMTUSDM - wheat / month\n",
        "  # PSOYBUSDM - soybeans / month\n",
        "\n",
        "  # daily data\n",
        "  dailySeries = ['T10YFF', 'T10Y2Y', 'BAMLCC7A01015YTRIV', 'DBAA', 'NASDAQCOM', \n",
        "                 'SP500', 'DCOILWTICO', 'DCOILBRENTEU',  \n",
        "                 'DHHNGSP', 'DJFUELUSGULF', 'DGASUSGULF']\n",
        "\n",
        "  for timeSeries in dailySeries: \n",
        "        try:\n",
        "            df = pd.DataFrame(fred.get_series(timeSeries, observation_start='1962-01-01', observation_end=datetime.date.today()))\n",
        "            df.columns = [timeSeries]\n",
        "            df[timeSeries+'PctDaily'] = df[timeSeries].pct_change()\n",
        "            df[timeSeries+'60dChange'] = df[timeSeries+'PctDaily'].rolling(60, min_periods=2).apply(total_return_from_returns)\n",
        "            df[timeSeries+'20dChange'] = df[timeSeries+'PctDaily'].rolling(20, min_periods=2).apply(total_return_from_returns)\n",
        "            df['date'] = df.index\n",
        "            df = df.drop(timeSeries+'PctDaily', axis=1)\n",
        "            quotes_df = quotes_df.merge(df, on='date', how='left')\n",
        "        except:\n",
        "            print('skip', timeSeries)\n",
        "\n",
        "  # monthly data\n",
        "  for timeSeries in ['FEDFUNDS', 'PMETAINDEXM', 'PCOPPUSDM', 'PALLFNFINDEXM', 'PIORECRUSDM', 'PWHEAMTUSDM', 'PSOYBUSDM']:\n",
        "        try:\n",
        "            df_series = pd.DataFrame(fred.get_series(timeSeries, frequency = 'm', observation_start='1962-01-01', observation_end=datetime.date.today()))\n",
        "            df_series.index = df_series.index.to_period('M').to_timestamp('M')\n",
        "            df_series.columns = [timeSeries]\n",
        "            df_series[timeSeries+'PctMonth'] = df_series[timeSeries].pct_change()\n",
        "            df_series[timeSeries+'Pct3Month'] = df_series[timeSeries+'PctMonth'].rolling(3, min_periods=2).apply(total_return_from_returns)\n",
        "            df_series[timeSeries+'Pct6Month'] = df_series[timeSeries+'PctMonth'].rolling(6, min_periods=2).apply(total_return_from_returns)\n",
        "            df_series['date'] = df_series.index\n",
        "\n",
        "            df = pd.DataFrame(pd.date_range(start='1962-01-01', end=datetime.date.today()))\n",
        "            df.columns = ['date']\n",
        "            df = df.merge(df_series, how='left', on='date')\n",
        "            df = df.fillna(method='ffill')\n",
        "            quotes_df = quotes_df.merge(df, on='date', how='left')\n",
        "        except:\n",
        "            print('skip', timeSeries)\n",
        "\n",
        "  return reduce_mem_usage(quotes_df)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thxKVbS1QKQ0"
      },
      "source": [
        "# Подготовка таблицы с данными по котировкам для расчета индикаторов\n",
        "with open('{}historical_price_last_year.pickle'.format(DIRECTORY), 'rb') as f: \n",
        "  quotes = pickle.load(f)[['date', 'symbol', 'volume', 'close', 'vwap']]\n",
        "\n",
        "quotes = quotes.drop_duplicates(subset=['date', 'symbol'])\n",
        "quotes = quotes[quotes['close']>0]\n",
        "quotes = quotes.sort_values(by=['symbol', 'date']) # теперь старые даты сверху\n",
        "quotes = quotes.reset_index(drop=True)\n",
        "quotes['daily_pct_change'] = daily_returns(quotes[['symbol', 'close']]) # добавляем дневное изменение расчитанное по клоузам, т.к. API дает дневное изменение как (C-O)/O \n",
        "quotes = next_month_return_and_target_alt(quotes) # добавляет изменение % за след. календарный месяц, отмечает категории таргета\n",
        "quotes = business_days(quotes) # отмечаем последние дни торгового месяца - будет использоваться для формирования прогноза на эти даты\n",
        "quotes = reduce_mem_usage(quotes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZvoLKQOQyAZ"
      },
      "source": [
        "# загружаем pl, bs, fin growth, key ratios\n",
        "quotes = load_fundamentals(quotes) # добавляем фундаментальные данные\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "\n",
        "# поскольку смердженные фундаменталки содержат кучу na - делаем ffill чтобы их заполнить\n",
        "fund_cols = ['period_end','fillingDate', 'acceptedDate', 'ebitdaratio', 'operatingIncomeRatio', 'incomeBeforeTaxRatio',\n",
        " 'netIncomeRatio', 'eps', 'weightedAverageShsOut', 'ebitda', 'incomeBeforeTax','interestExpense',\n",
        " 'revenue', 'revenueGrowth', 'grossProfitGrowth', 'ebitgrowth','operatingIncomeGrowth', 'netIncomeGrowth',\n",
        " 'epsgrowth', 'epsdilutedGrowth', 'weightedAverageSharesGrowth', 'weightedAverageSharesDilutedGrowth',\n",
        " 'dividendsperShareGrowth','operatingCashFlowGrowth', 'freeCashFlowGrowth', 'tenYRevenueGrowthPerShare', 'fiveYRevenueGrowthPerShare',\n",
        " 'threeYRevenueGrowthPerShare','tenYOperatingCFGrowthPerShare', 'fiveYOperatingCFGrowthPerShare', 'threeYOperatingCFGrowthPerShare',\n",
        " 'tenYNetIncomeGrowthPerShare', 'fiveYNetIncomeGrowthPerShare', 'threeYNetIncomeGrowthPerShare', 'tenYShareholdersEquityGrowthPerShare',\n",
        " 'fiveYShareholdersEquityGrowthPerShare', 'threeYShareholdersEquityGrowthPerShare','tenYDividendperShareGrowthPerShare',\n",
        " 'fiveYDividendperShareGrowthPerShare', 'threeYDividendperShareGrowthPerShare', 'receivablesGrowth', 'inventoryGrowth',\n",
        " 'assetGrowth', 'bookValueperShareGrowth', 'debtGrowth', 'rdexpenseGrowth', 'sgaexpensesGrowth', 'revenuePerShare',\n",
        " 'netIncomePerShare', 'operatingCashFlowPerShare', 'freeCashFlowPerShare', 'cashPerShare', 'bookValuePerShare',\n",
        " 'tangibleBookValuePerShare', 'shareholdersEquityPerShare', 'interestDebtPerShare', 'marketCap', 'enterpriseValue', 'peRatio',\n",
        " 'priceToSalesRatio', 'pocfratio', 'pfcfRatio', 'pbRatio', 'ptbRatio', 'evToSales', 'enterpriseValueOverEBITDA',\n",
        " 'evToOperatingCashFlow', 'evToFreeCashFlow', 'earningsYield', 'freeCashFlowYield','debtToEquity', 'debtToAssets', 'netDebtToEBITDA',\n",
        " 'currentRatio', 'interestCoverage', 'incomeQuality', 'dividendYield', 'payoutRatio', 'salesGeneralAndAdministrativeToRevenue',\n",
        " 'researchAndDdevelopementToRevenue', 'intangiblesToTotalAssets', 'capexToOperatingCashFlow', 'capexToRevenue',\n",
        " 'capexToDepreciation', 'stockBasedCompensationToRevenue', 'grahamNumber', 'roic', 'returnOnTangibleAssets', 'grahamNetNet',\n",
        " 'workingCapital', 'tangibleAssetValue', 'netCurrentAssetValue', 'investedCapital', 'averageReceivables', 'averagePayables',\n",
        " 'averageInventory', 'daysSalesOutstanding', 'daysPayablesOutstanding', 'daysOfInventoryOnHand', 'receivablesTurnover',\n",
        " 'payablesTurnover', 'inventoryTurnover', 'roe', 'capexPerShare','totalStockholdersEquity','retainedEarnings','longTermDebt']\n",
        "\n",
        "for col in fund_cols:\n",
        "    try:\n",
        "        quotes[col] = quotes[[col, 'symbol']].groupby('symbol').fillna(method='ffill')[col]\n",
        "    except:\n",
        "        quotes[col] = quotes[col].astype(float)\n",
        "        quotes[col] = quotes[[col, 'symbol']].groupby('symbol').fillna(method='ffill')[col]\n",
        "\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(quotes.memory_usage(index=True).sum())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCryCoouQnaB"
      },
      "source": [
        "quotes['LAST_YEAR_PERFORMANCE'] = ROC(quotes[['symbol', 'close']], period=251)\n",
        "quotes = load_profile(quotes) # добавляем данные по профайлу компании\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "quotes = fill_shares_number(quotes) # не везде заполнено количество акций, поэтому высчитываем этот показатель на основе выручки и выручки\\акция\n",
        "quotes['p_b_ratio_daily_alter'] = p_b_ratio_daily_alter(quotes[['close', 'bookValuePerShare']])\n",
        "quotes['net_profit_to_total_operate_revenue_ttm'] = net_profit_to_total_operate_revenue_ttm(quotes[['netIncomePerShare', 'revenuePerShare']])\n",
        "quotes['market_cap_daily'] = market_cap_daily(quotes[['weightedAverageShsOut', 'close']])\n",
        "quotes['VOL20'] = VOL(quotes[['symbol', 'volume', 'weightedAverageShsOut', 'vwap']])\n",
        "quotes['DAVOL20'] = DAVOL(quotes[['symbol', 'volume', 'weightedAverageShsOut', 'vwap']])\n",
        "quotes['retained_earnings_per_share'] = retained_earnings_per_share(quotes[['retainedEarnings', 'weightedAverageShsOut']])\n",
        "quotes['cash_flow_to_price_ratio'] = cash_flow_to_price_ratio(quotes[['freeCashFlowPerShare', 'close']])\n",
        "quotes['financial_liability_to_price'] = financial_liability_to_price(quotes[['longTermDebt', 'close', 'weightedAverageShsOut']])\n",
        "quotes['ACCA'] = ACCA(quotes[['freeCashFlowPerShare', 'p_b_ratio_daily_alter']])\n",
        "quotes['Price_index_6m'] = ROC(quotes[['symbol', 'close']], period=int(252/2)) #измненеие цены за последние 6м"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RhO0aj-RAKH"
      },
      "source": [
        "# ###########\n",
        "# Трехфакторная модель Ferma-French (без рыночной беты пока что)\n",
        "quotes[['ff_groups_book_median', 'ff_groups_profit_median']] = set_ff_groups_median(quotes) # set farma french 3 factor model groups \n",
        "quotes = reduce_mem_usage(quotes)\n",
        "quotes = calculate_ff_group_returns(quotes)\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "quotes['VARIANCE20_ff_groups_profit_median'] = VARIANCE(quotes[['symbol', 'daily_pct_change_weighted_by_cap_ff_groups_profit_median']], column='daily_pct_change_weighted_by_cap_ff_groups_profit_median')\n",
        "quotes['VARIANCE20_ff_groups_book_median'] = VARIANCE(quotes[['symbol', 'daily_pct_change_weighted_by_cap_ff_groups_book_median']], column='daily_pct_change_weighted_by_cap_ff_groups_book_median')\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "quotes['diff_ff_group_profit_251_return_and_stock_251_return'] = (quotes['LAST_YEAR_PERFORMANCE'] - quotes['pct_change_ff_groups_profit_median_251']) / quotes['pct_change_ff_groups_profit_median_251']\n",
        "quotes['diff_ff_group_book_251_return_and_stock_251_return'] = (quotes['LAST_YEAR_PERFORMANCE'] - quotes['pct_change_ff_groups_book_median_251']) / quotes['pct_change_ff_groups_book_median_251']\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "\n",
        "## quantum fundamental factors\n",
        "quotes['sales_to_price_daily'] = sales_to_price_daily(quotes[['revenue', 'weightedAverageShsOut', 'close']])\n",
        "quotes['earnings_to_price_daily'] = earnings_to_price_ratio(quotes[['incomeBeforeTax', 'weightedAverageShsOut', 'close']])\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "## Ranking\n",
        "quotes['rank_pb_value'] = calculate_rank(quotes[['date', 'p_b_ratio_daily_alter']], column_to_rank='p_b_ratio_daily_alter', group_by_columns='date')\n",
        "quotes['rank_Price_index_6m'] = calculate_rank(quotes[['date', 'Price_index_6m']], column_to_rank='Price_index_6m', group_by_columns='date')\n",
        "quotes['rank_dividendYield'] = calculate_rank(quotes[['date', 'dividendYield']], column_to_rank='dividendYield', group_by_columns='date')\n",
        "quotes['rank_enterpriseValueOverEBITDA'] = calculate_rank(quotes[['date', 'enterpriseValueOverEBITDA']], column_to_rank='enterpriseValueOverEBITDA', group_by_columns='date')\n",
        "quotes['rank_sales_to_price_daily'] = calculate_rank(quotes[['date', 'sales_to_price_daily']], column_to_rank='sales_to_price_daily', group_by_columns='date')\n",
        "quotes['rank_earnings_to_price_daily'] = calculate_rank(quotes[['date', 'earnings_to_price_daily']], column_to_rank='earnings_to_price_daily', group_by_columns='date')\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "## total rank stratagies\n",
        "## priceindex 6m + booktoprice\n",
        "quotes['rank_first_strategie_value'] = quotes['rank_pb_value'] + quotes['rank_Price_index_6m']\n",
        "quotes['rank_first_strategie'] = calculate_rank(quotes[['date', 'rank_first_strategie_value']], column_to_rank='rank_first_strategie_value', group_by_columns='date')\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "# ## p/b+ p/s + ebitda/ev + p/e + div yield\n",
        "# # OShaughnessy trending value investment strategy\n",
        "# # https://www.quant-investing.com/strategies/oshaughnessy-trending-value-investment-strategy\n",
        "# # Тут не хватает Price to cash flow!!!\n",
        "quotes['rank_second_strategie_value'] = quotes['rank_pb_value'] + quotes['rank_sales_to_price_daily'] + quotes['rank_enterpriseValueOverEBITDA'] + quotes['rank_earnings_to_price_daily'] + quotes['rank_dividendYield']\n",
        "quotes['rank_second_strategie'] = calculate_rank(quotes[['date', 'rank_second_strategie_value']], column_to_rank='rank_second_strategie_value', group_by_columns='date')\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "#####################\n",
        "\n",
        "# TO DO:\n",
        "# 1) Qi Value Investment Strategy: EBITDA Yield + FCF Yield + Liquidity (Q.i) is calculated as Adjusted Profits / Yearly trading value.\n",
        "# https://www.quant-investing.com/strategies/qi-value-investment-strategy\n",
        "\n",
        "# 2) Quality Adjusted Value Microcap Investment Strategy - OShaughnessy\n",
        "# https://www.quant-investing.com/strategies/quality-adjusted-value-microcap-investment-strategy---oshaughnessy\n",
        "# o do this they removed companies that fell in the bottom 10% of the following five ratios or indicators:\n",
        "# Change in Net Operating Assets (NOA = Operating Assets - Operating Liabilities)\n",
        "# Debt to Equity Ratio\n",
        "# 1-Year change in debt\n",
        "# Return on Invested Capital (ROIC)\n",
        "# Free cash flow yield\n",
        "# The portfolios were re-balanced yearly and the portfolios were equal-weighted.\n",
        "\n",
        "# 3) Free cash flow yield and Price Index 12 months investment strategy\n",
        "# 4) https://www.quant-investing.com/strategies/neglected-value-and-momentum---europe\n",
        "\n",
        "# https://virtusinterpress.org/IMG/pdf/cocv15i1art4.pdf\n",
        "# We used the same market indicators of Tsuji (2012): inflation, credit risk, \n",
        "# dividend yield and volatility of market return. The inflation was measured \n",
        "# through Consumer Price Index (IPCA). The credit risk was calculated through \n",
        "# the difference between the working capital interest rate and the interbank \n",
        "# market interest rate (Schor et al., 2002). The market dividend yield is \n",
        "# the average dividend yield (dividend of the last 12 months divided by the \n",
        "# share price) of the companies in the sample weighted by their market value. \n",
        "# Market volatility was calculated by the one-year standard deviation of the \n",
        "# stock market index IBrX-100, and the country risk was measured by EMBI + Brazil index.\n",
        "#  We also add in the regressions an overall corporate governance measure, \n",
        "#  proxied by the percentage of companies listed on “Novo Mercado” (New Market).\n",
        "\n",
        "# 5) Earnings estimates count - как много аналитиков покрывают компанию, больше - лучше\n",
        "# 6) N-month change in recommendation - This factor ranks stocks by the change in consensus\n",
        "# recommendation over the prior N month, where improvements are desirable (regardless of whether\n",
        "# they have moved from strong sell to sell or buy to strong buy and so on).\n",
        "# 7)12-month change in shares outstanding This factor measures the change in a company's \n",
        "# split-adjusted share count over the last 12 months, where a negative change implies\n",
        "# share buybacks and is desirable because it signals that management views the stock\n",
        "# as cheap relative to its intrinsic and, hence, future value.\n",
        "# 8) The metric tracks the 6-month change in mean analyst target price. A higher positive change is naturally more desirable.\n",
        "# 9) net earnings revisions - This factor expresses the difference between upward and downward revisions to earnings estimates as a percentage of the total number of revisions.\n",
        "# 10) Short interest to shares outstanding - This measure is the percentage of shares\n",
        "# outstanding currently being sold short, that is, sold by an investor who ha\n",
        "# borrowed the share and needs to repurchase it at a later day while speculating that \n",
        "# its price will fall. Hence, a high level of short interest indicates negative sentiment \n",
        "# and is expected to signal poor performance going forward.\n",
        "# 11) The price/earnings to growth (PEG) ratio divides a stock's price-to- earnings (P/E) \n",
        "# ratio by the earnings growth rate for a given period. The ratio adjusts the price paid for a \n",
        "# dollar of earnings (measured by the P/E ratio) by the company's earnings growth.\n",
        "# 12) Earnings yield 1- year forward\n",
        "# 13) higher total accruals relative to assets make low earnings quality more likely\n",
        "# 14) asset turnover dividing sales by total assets. A higher turnover is better.\n",
        "# 15) asset turnover 12month change\n",
        "# 16) payout ratio - The share of earnings paid out in dividends to shareholders. Stocks with higher payout ratios are ranked higher.\n",
        "# 17) 101 alpha factors to use: https://arxiv.org/pdf/1601.00991.pdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-JitowgRPat"
      },
      "source": [
        "# Технические индикаторы\n",
        "\n",
        "quotes['VOSC'] = VOSC(quotes[['symbol', 'volume']])\n",
        "quotes['VMACD'] = VMACD2(quotes[['symbol', 'volume', 'date']])\n",
        "quotes['ROC'] = ROC(quotes[['symbol', 'close']])\n",
        "quotes['VOLUME1M'] = VOLUME(quotes[['symbol', 'volume', 'ROC']])\n",
        "quotes['Price1M'] = Price1M(quotes[['symbol', 'close']])\n",
        "quotes['MAC20'] = MAC(quotes[['symbol', 'close']])\n",
        "quotes['VARIANCE20'] = VARIANCE(quotes[['symbol', 'daily_pct_change']])\n",
        "quotes['KURTOSIS20'] = KURTOSIS(quotes[['symbol', 'close']])\n",
        "quotes['SKEW20'] = SKEW(quotes[['symbol', 'close']])\n",
        "quotes['TRIX10'] = TRIX(quotes[['symbol', 'close']])\n",
        "# # quotes['MFI14'] = MFI(quotes[['symbol', 'close', 'low', 'high', 'volume']]) # очень долго считается!\n",
        "quotes['BOLL_DOWN20'] = boll_down(quotes[['symbol', 'close']])\n",
        "quotes['BOLL_UP20'] = boll_up(quotes[['symbol', 'close']])\n",
        "quotes['SHARP20'] = SHARPE_RATIO(quotes[['symbol', 'close']], period=20)\n",
        "quotes['SHARP60'] = SHARPE_RATIO(quotes[['symbol', 'close']], period=60)\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "\n",
        "# to do:\n",
        "# 1) RSI\n",
        "# 2) 12-month price momentum volume adjustment. The indicator normalizes the total return over the previous 12 months by volume dividing it by the standard deviation of these returns.\n",
        "# 3) Price acceleration calculates the gradient of the price trend (adjusted for volatility) \n",
        "# using linear regression on daily prices for a longer and a shorter period, \n",
        "# for example, 1 year and 3 months of trading days, and compares the change in the slope as a measure of price acceleration.\n",
        "# 4) Percent off 52-week high\n",
        "# 5) use talib lib! \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyeyTndARXzE"
      },
      "source": [
        "# # # Alternative DATA\n",
        "words, quotes = google_trends(quotes) # добавлем историю поиска по гугл трендс\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "quotes = load_FRED_data(quotes) # см список загружаемых данных по макро с сайта FRED внутри функции\n",
        "quotes = reduce_mem_usage(quotes)\n",
        "quotes = load_quandl(quotes) # подгружаем с квандла золото, потому что фред не хотел отдавать золото по api\n",
        "quotes = reduce_mem_usage(quotes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_vRoEEmRna0"
      },
      "source": [
        "# Сохраняем датасет по кускам\n",
        "\n",
        "base_cols = ['date','symbol','volume', 'close', 'vwap','daily_pct_change','year',\n",
        " 'month','price_change_next_calendar_month','target','last_bday_current_month','is_last_bday_current_month',]\n",
        "\n",
        "fund_cols1 = ['date','symbol','period_end','fillingDate', 'acceptedDate',\n",
        " 'ebitdaratio', 'operatingIncomeRatio', 'incomeBeforeTaxRatio', 'netIncomeRatio', 'eps', 'weightedAverageShsOut', 'ebitda',\n",
        " 'incomeBeforeTax','interestExpense', 'revenue', 'revenueGrowth', 'grossProfitGrowth', 'ebitgrowth', 'operatingIncomeGrowth',\n",
        " 'netIncomeGrowth', 'epsgrowth', 'epsdilutedGrowth', 'weightedAverageSharesGrowth', 'weightedAverageSharesDilutedGrowth',\n",
        " 'dividendsperShareGrowth', 'operatingCashFlowGrowth', 'freeCashFlowGrowth', 'tenYRevenueGrowthPerShare','fiveYRevenueGrowthPerShare',\n",
        " 'threeYRevenueGrowthPerShare', 'tenYOperatingCFGrowthPerShare','fiveYOperatingCFGrowthPerShare','threeYOperatingCFGrowthPerShare',]\n",
        "\n",
        "fund_cols2 =['date','symbol',\n",
        "     'tenYNetIncomeGrowthPerShare','fiveYNetIncomeGrowthPerShare', 'threeYNetIncomeGrowthPerShare',\n",
        " 'tenYShareholdersEquityGrowthPerShare', 'fiveYShareholdersEquityGrowthPerShare', 'threeYShareholdersEquityGrowthPerShare',\n",
        " 'tenYDividendperShareGrowthPerShare', 'fiveYDividendperShareGrowthPerShare', 'threeYDividendperShareGrowthPerShare',\n",
        " 'receivablesGrowth', 'inventoryGrowth', 'assetGrowth', 'bookValueperShareGrowth', 'debtGrowth', 'rdexpenseGrowth',\n",
        " 'sgaexpensesGrowth', 'revenuePerShare','netIncomePerShare', 'operatingCashFlowPerShare', 'freeCashFlowPerShare',\n",
        " 'cashPerShare', 'bookValuePerShare', 'tangibleBookValuePerShare', 'shareholdersEquityPerShare', 'interestDebtPerShare',\n",
        " 'marketCap', 'enterpriseValue', 'peRatio', 'priceToSalesRatio', 'pocfratio', 'pfcfRatio', 'pbRatio', 'ptbRatio', 'evToSales',\n",
        "]\n",
        "\n",
        "fund_cols3 = ['date','symbol',\n",
        "    'enterpriseValueOverEBITDA', 'evToOperatingCashFlow','evToFreeCashFlow',\n",
        " 'earningsYield', 'freeCashFlowYield', 'debtToEquity', 'debtToAssets','netDebtToEBITDA', 'currentRatio',\n",
        " 'interestCoverage', 'incomeQuality', 'dividendYield', 'payoutRatio','salesGeneralAndAdministrativeToRevenue',\n",
        " 'researchAndDdevelopementToRevenue', 'intangiblesToTotalAssets', 'capexToOperatingCashFlow', 'capexToRevenue', 'capexToDepreciation',\n",
        " 'stockBasedCompensationToRevenue', 'grahamNumber', 'roic', 'returnOnTangibleAssets', 'grahamNetNet', 'workingCapital',\n",
        " 'tangibleAssetValue', 'netCurrentAssetValue', 'investedCapital','averageReceivables', 'averagePayables',\n",
        " 'averageInventory', 'daysSalesOutstanding', 'daysPayablesOutstanding', 'daysOfInventoryOnHand', 'receivablesTurnover',\n",
        " 'payablesTurnover', 'inventoryTurnover', 'roe', 'capexPerShare', 'totalStockholdersEquity', 'retainedEarnings', 'longTermDebt'\n",
        "]\n",
        "\n",
        "profile_cols = ['date','symbol','industry','ceo','sector', 'fullTimeEmployees','isActivelyTrading','isEtf',]\n",
        "\n",
        "quant_cols = ['date','symbol','p_b_ratio_daily_alter',\n",
        " 'net_profit_to_total_operate_revenue_ttm', 'market_cap_daily','retained_earnings_per_share',\n",
        " 'cash_flow_to_price_ratio', 'financial_liability_to_price','sales_to_price_daily',\n",
        " 'earnings_to_price_daily', 'rank_pb_value', 'Price_index_6m',\n",
        " 'rank_Price_index_6m', 'rank_dividendYield', 'rank_enterpriseValueOverEBITDA',\n",
        " 'rank_sales_to_price_daily', 'rank_earnings_to_price_daily', 'rank_first_strategie_value',\n",
        " 'rank_first_strategie', 'rank_second_strategie_value','rank_second_strategie',]\n",
        "\n",
        "ff_cols = ['date','symbol','ff_groups_book_median',\n",
        " 'ff_groups_profit_median', 'total_cap_ff_groups_book_median', 'daily_pct_change_weighted_by_cap_ff_groups_book_median',\n",
        " 'pct_change_ff_groups_book_median_251', 'bh', 'bl', 'sh', 'sl', 'HML', 'SMB',\n",
        " 'pct_change_ff_groups_book_median_60', 'pct_change_ff_groups_book_median_20', 'total_cap_ff_groups_profit_median',\n",
        " 'daily_pct_change_weighted_by_cap_ff_groups_profit_median', 'pct_change_ff_groups_profit_median_251', 'sr', 'br', 'sw', 'bw','RMW', 'pct_change_ff_groups_profit_median_60',\n",
        " 'pct_change_ff_groups_profit_median_20', 'VARIANCE20_ff_groups_profit_median','VARIANCE20_ff_groups_book_median',\n",
        " 'diff_ff_group_profit_251_return_and_stock_251_return','diff_ff_group_book_251_return_and_stock_251_return',]\n",
        "\n",
        "indeicators_cols = ['date','symbol', 'LAST_YEAR_PERFORMANCE','VOL20','DAVOL20', 'ACCA','VOSC',\n",
        " 'VMACD', 'ROC', 'VOLUME1M', 'Price1M', 'MAC20', 'VARIANCE20', 'KURTOSIS20', 'SKEW20',\n",
        " 'TRIX10', 'BOLL_DOWN20', 'BOLL_UP20', 'SHARP20', 'SHARP60']\n",
        "\n",
        "google_cols = ['date','symbol', 'debt', 'color', 'stocks', 'restaurant', 'portfolio', 'inflation', 'housing',\n",
        " 'ring', 'crisis financial', 'stock crash', 'labour', 'stock market', 'buy and hold','unemployment']\n",
        "\n",
        "fred_cols = ['date', 'symbol', 'T10YFF',\n",
        "       'T10YFF60dChange', 'T10YFF20dChange', 'T10Y2Y', 'T10Y2Y60dChange',\n",
        "       'T10Y2Y20dChange', 'BAMLCC7A01015YTRIV', 'BAMLCC7A01015YTRIV60dChange',\n",
        "       'BAMLCC7A01015YTRIV20dChange', 'DBAA', 'DBAA60dChange', 'DBAA20dChange',\n",
        "       'NASDAQCOM', 'NASDAQCOM60dChange', 'NASDAQCOM20dChange', 'SP500',\n",
        "       'SP50060dChange', 'SP50020dChange', 'DCOILWTICO', 'DCOILWTICO60dChange',\n",
        "       'DCOILWTICO20dChange', 'DCOILBRENTEU', 'DCOILBRENTEU60dChange',\n",
        "       'DCOILBRENTEU20dChange', 'DHHNGSP', 'DHHNGSP60dChange',\n",
        "       'DHHNGSP20dChange', 'DJFUELUSGULF', 'DJFUELUSGULF60dChange',\n",
        "       'DJFUELUSGULF20dChange', 'DGASUSGULF', 'DGASUSGULF60dChange',\n",
        "       'DGASUSGULF20dChange', 'FEDFUNDS', 'FEDFUNDSPctMonth',\n",
        "       'FEDFUNDSPct3Month', 'FEDFUNDSPct6Month', 'PMETAINDEXM',\n",
        "       'PMETAINDEXMPctMonth', 'PMETAINDEXMPct3Month', 'PMETAINDEXMPct6Month',\n",
        "       'PCOPPUSDM', 'PCOPPUSDMPctMonth', 'PCOPPUSDMPct3Month',\n",
        "       'PCOPPUSDMPct6Month', 'PALLFNFINDEXM', 'PALLFNFINDEXMPctMonth',\n",
        "       'PALLFNFINDEXMPct3Month', 'PALLFNFINDEXMPct6Month', 'PIORECRUSDM',\n",
        "       'PIORECRUSDMPctMonth', 'PIORECRUSDMPct3Month', 'PIORECRUSDMPct6Month',\n",
        "       'PWHEAMTUSDM', 'PWHEAMTUSDMPctMonth', 'PWHEAMTUSDMPct3Month',\n",
        "       'PWHEAMTUSDMPct6Month', 'PSOYBUSDM', 'PSOYBUSDMPctMonth',\n",
        "       'PSOYBUSDMPct3Month', 'PSOYBUSDMPct6Month']\n",
        "\n",
        "quandl_cols = ['date', 'symbol', 'LBMA/GOLD',\n",
        "       'LBMA/GOLD60dChange', 'LBMA/GOLD20dChange']\n",
        "\n",
        "# сохраняем датасет  кусками\n",
        "with open('{}base_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[base_cols], f)\n",
        "\n",
        "with open('{}fundamental1_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[fund_cols1], f)\n",
        "\n",
        "with open('{}fundamental2_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[fund_cols2], f)\n",
        "\n",
        "with open('{}fundamental3_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[fund_cols3], f)\n",
        "\n",
        "with open('{}profile_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[profile_cols], f)\n",
        "\n",
        "with open('{}quant_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[quant_cols], f)\n",
        "\n",
        "with open('{}ff_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[ff_cols], f)\n",
        "\n",
        "with open('{}indicators_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[indeicators_cols], f)\n",
        "\n",
        "with open('{}google_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[google_cols], f)\n",
        "\n",
        "with open('{}fred_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[fred_cols], f)\n",
        "\n",
        "with open('{}quandl_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes[quandl_cols], f)\n",
        "\n",
        "del quotes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM44cfSfSu59"
      },
      "source": [
        "### Объединение датасета в одну таблицу и фильтр только последних дней месяца - только в них будет торговля, все остальные нам не нужны\n",
        "\n",
        "df_names = ['fundamental1_df', 'fundamental2_df',\n",
        "'fundamental3_df','profile_df','quant_df','ff_df','indicators_df','google_df','fred_df','quandl_df',]\n",
        "skip_cols = ['date', 'symbol']\n",
        "\n",
        "with open('{}base_df.pickle'.format(DIRECTORY), 'rb') as f: \n",
        "  quotes = pickle.load(f)\n",
        "\n",
        "mask_end_month = quotes['is_last_bday_current_month']==True\n",
        "quotes = quotes[mask_end_month]\n",
        "\n",
        "for df_name in df_names:\n",
        "    with open('{}{}.pickle'.format(DIRECTORY, df_name), 'rb') as f: \n",
        "      df = pickle.load(f)[mask_end_month]\n",
        "    df = df[list(set(df.columns)-set(skip_cols))]\n",
        "    quotes = quotes.merge(df, left_index=True, right_index=True)\n",
        "\n",
        "with open('{}final_df.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(quotes, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoqHWnddW2-G"
      },
      "source": [
        "## ОБУЧЕНИЕ И ТЕСТ МОДЕЛИ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsuqXNvlW7BY"
      },
      "source": [
        "def cal_beta(strategy):\n",
        "  nasdaq = quotes[['date','NASDAQCOM']].drop_duplicates().sort_values(by=['date']).set_index('date').dropna()\n",
        "  nasdaq['nasdaq_pct_change'] = nasdaq['NASDAQCOM'].astype(float).pct_change()\n",
        "  strategy.columns = ['strategy']\n",
        "  strategy['strategy'] = strategy['strategy'].shift(1)\n",
        "  strategy['strategy_pct_chage'] = strategy['strategy'].pct_change()\n",
        "  strategy = strategy.merge(nasdaq, how='left', left_index=True, right_index=True)\n",
        "  strategy = strategy.dropna()\n",
        "  model = LinearRegression()\n",
        "  model.fit(strategy['nasdaq_pct_change'].values.reshape(-1, 1), strategy['strategy_pct_chage'].values.reshape(-1, 1))\n",
        "  beta = model.coef_[0][0]\n",
        "  return beta"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwowDM8RXNij"
      },
      "source": [
        "# Загружаем финальный датасет для обучения\n",
        "with open('{}final_df.pickle'.format(DIRECTORY), 'rb') as f: \n",
        "  quotes = pickle.load(f)\n",
        "\n",
        "skip_symbols = quotes[quotes['price_change_next_calendar_month']>5]['symbol'].unique() # скипаем тикеры с глючками в close (поставщик данных прокосячил на парочке)\n",
        "len(quotes[-quotes['symbol'].isin(skip_symbols)]['symbol'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IlIHWCUXWfx"
      },
      "source": [
        "features_columns = [\n",
        " 'fiveYRevenueGrowthPerShare',\n",
        " 'threeYRevenueGrowthPerShare',\n",
        " 'grossProfitGrowth',\n",
        " 'ebitda',\n",
        " 'weightedAverageSharesGrowth',\n",
        " 'tenYRevenueGrowthPerShare',\n",
        " 'epsgrowth',\n",
        " 'operatingCashFlowGrowth',\n",
        " 'tenYOperatingCFGrowthPerShare',\n",
        " 'freeCashFlowGrowth',\n",
        " 'ebitgrowth',\n",
        " 'revenue',\n",
        " 'operatingIncomeRatio',\n",
        " 'eps',\n",
        " 'epsdilutedGrowth',\n",
        " 'netIncomeGrowth',\n",
        " 'weightedAverageSharesDilutedGrowth',\n",
        " 'incomeBeforeTaxRatio',\n",
        " 'operatingIncomeGrowth',\n",
        " 'incomeBeforeTax',\n",
        " 'ebitdaratio',\n",
        " 'dividendsperShareGrowth',\n",
        " 'netIncomeRatio',\n",
        " 'threeYOperatingCFGrowthPerShare',\n",
        " 'weightedAverageShsOut',\n",
        " 'revenueGrowth',\n",
        " 'interestExpense',\n",
        " 'fiveYOperatingCFGrowthPerShare',\n",
        " 'rdexpenseGrowth',\n",
        " 'bookValuePerShare',\n",
        " 'marketCap',\n",
        " 'tenYShareholdersEquityGrowthPerShare',\n",
        " 'fiveYDividendperShareGrowthPerShare',\n",
        " 'interestDebtPerShare',\n",
        " 'tenYDividendperShareGrowthPerShare',\n",
        " 'shareholdersEquityPerShare',\n",
        " 'inventoryGrowth',\n",
        " 'tangibleBookValuePerShare',\n",
        " 'threeYNetIncomeGrowthPerShare',\n",
        " 'receivablesGrowth',\n",
        " 'priceToSalesRatio',\n",
        " 'pfcfRatio',\n",
        " 'sgaexpensesGrowth',\n",
        " 'threeYDividendperShareGrowthPerShare',\n",
        " 'fiveYShareholdersEquityGrowthPerShare',\n",
        " 'assetGrowth',\n",
        " 'revenuePerShare',\n",
        " 'debtGrowth',\n",
        " 'freeCashFlowPerShare',\n",
        " 'pocfratio',\n",
        " 'pbRatio',\n",
        " 'fiveYNetIncomeGrowthPerShare',\n",
        " 'threeYShareholdersEquityGrowthPerShare',\n",
        " 'tenYNetIncomeGrowthPerShare',\n",
        " 'bookValueperShareGrowth',\n",
        " 'evToSales',\n",
        " 'netIncomePerShare',\n",
        " 'enterpriseValue',\n",
        " 'operatingCashFlowPerShare',\n",
        " 'cashPerShare',\n",
        " 'ptbRatio',\n",
        " 'peRatio',\n",
        " 'retainedEarnings',\n",
        " 'daysSalesOutstanding',\n",
        " 'debtToEquity',\n",
        " 'longTermDebt',\n",
        " 'earningsYield',\n",
        " 'inventoryTurnover',\n",
        " 'returnOnTangibleAssets',\n",
        " 'averagePayables',\n",
        " 'daysPayablesOutstanding',\n",
        " 'interestCoverage',\n",
        " 'freeCashFlowYield',\n",
        " 'investedCapital',\n",
        " 'netCurrentAssetValue',\n",
        " 'salesGeneralAndAdministrativeToRevenue',\n",
        " 'workingCapital',\n",
        " 'incomeQuality',\n",
        " 'currentRatio',\n",
        " 'enterpriseValueOverEBITDA',\n",
        " 'averageReceivables',\n",
        " 'roe',\n",
        " 'grahamNumber',\n",
        " 'stockBasedCompensationToRevenue',\n",
        " 'daysOfInventoryOnHand',\n",
        " 'evToOperatingCashFlow',\n",
        " 'dividendYield',\n",
        " 'payablesTurnover',\n",
        " 'researchAndDdevelopementToRevenue',\n",
        " 'tangibleAssetValue',\n",
        " 'averageInventory',\n",
        " 'totalStockholdersEquity',\n",
        " 'capexToRevenue',\n",
        " 'roic',\n",
        " 'payoutRatio',\n",
        " 'capexToOperatingCashFlow',\n",
        " 'debtToAssets',\n",
        " 'evToFreeCashFlow',\n",
        " 'intangiblesToTotalAssets',\n",
        " 'netDebtToEBITDA',\n",
        " 'grahamNetNet',\n",
        " 'capexPerShare',\n",
        " 'receivablesTurnover',\n",
        " 'capexToDepreciation',\n",
        " 'industry',\n",
        " 'sector',\n",
        " 'fullTimeEmployees',\n",
        " 'rank_second_strategie_value',\n",
        " 'rank_sales_to_price_daily',\n",
        " 'rank_Price_index_6m',\n",
        " 'net_profit_to_total_operate_revenue_ttm',\n",
        " 'cash_flow_to_price_ratio',\n",
        " 'financial_liability_to_price',\n",
        " 'rank_dividendYield',\n",
        " 'p_b_ratio_daily_alter',\n",
        " 'rank_pb_value',\n",
        " 'rank_earnings_to_price_daily',\n",
        " 'market_cap_daily',\n",
        " 'rank_second_strategie',\n",
        " 'sales_to_price_daily',\n",
        " 'rank_first_strategie_value',\n",
        " 'rank_enterpriseValueOverEBITDA',\n",
        " 'rank_first_strategie',\n",
        " 'retained_earnings_per_share',\n",
        " 'Price_index_6m',\n",
        " 'earnings_to_price_daily',\n",
        " 'daily_pct_change_weighted_by_cap_ff_groups_book_median',\n",
        " 'pct_change_ff_groups_profit_median_20',\n",
        " 'sl',\n",
        " 'pct_change_ff_groups_profit_median_251',\n",
        " 'pct_change_ff_groups_book_median_60',\n",
        " 'pct_change_ff_groups_book_median_20',\n",
        " 'ff_groups_profit_median',\n",
        " 'sr',\n",
        " 'br',\n",
        " 'total_cap_ff_groups_book_median',\n",
        " 'diff_ff_group_book_251_return_and_stock_251_return',\n",
        " 'pct_change_ff_groups_profit_median_60',\n",
        " 'HML',\n",
        " 'bw',\n",
        " 'diff_ff_group_profit_251_return_and_stock_251_return',\n",
        " 'RMW',\n",
        " 'daily_pct_change_weighted_by_cap_ff_groups_profit_median',\n",
        " 'sw',\n",
        " 'bh',\n",
        " 'SMB',\n",
        " 'ff_groups_book_median',\n",
        " 'VARIANCE20_ff_groups_profit_median',\n",
        " 'pct_change_ff_groups_book_median_251',\n",
        " 'bl',\n",
        " 'sh',\n",
        " 'VARIANCE20_ff_groups_book_median',\n",
        " 'total_cap_ff_groups_profit_median',\n",
        " 'VOSC',\n",
        " 'VOL20',\n",
        " 'TRIX10',\n",
        " 'SHARP20',\n",
        " 'SHARP60',\n",
        " 'BOLL_DOWN20',\n",
        " 'Price1M',\n",
        " 'BOLL_UP20',\n",
        " 'VARIANCE20',\n",
        " 'ROC',\n",
        " 'VMACD',\n",
        " 'KURTOSIS20',\n",
        " 'ACCA',\n",
        " 'MAC20',\n",
        " 'DAVOL20',\n",
        " 'SKEW20',\n",
        " 'LAST_YEAR_PERFORMANCE',\n",
        " 'VOLUME1M',\n",
        " 'DHHNGSP60dChange',\n",
        " 'DGASUSGULF20dChange',\n",
        " 'DCOILBRENTEU60dChange',\n",
        " 'T10Y2Y20dChange',\n",
        " 'SP50060dChange',\n",
        " 'SP50020dChange',\n",
        " 'NASDAQCOM20dChange',\n",
        " 'DCOILBRENTEU20dChange',\n",
        " 'DHHNGSP20dChange',\n",
        " 'DJFUELUSGULF60dChange',\n",
        " 'DJFUELUSGULF20dChange',\n",
        " 'T10YFF20dChange',\n",
        " 'BAMLCC7A01015YTRIV60dChange',\n",
        " 'BAMLCC7A01015YTRIV20dChange',\n",
        " 'NASDAQCOM60dChange',\n",
        " 'DBAA60dChange',\n",
        " 'T10YFF60dChange',\n",
        " 'DBAA20dChange',\n",
        " 'DCOILWTICO60dChange',\n",
        " 'DCOILWTICO20dChange',\n",
        " 'T10Y2Y60dChange',\n",
        " 'DGASUSGULF60dChange',\n",
        " 'LBMA/GOLD20dChange',\n",
        " 'LBMA/GOLD60dChange'\n",
        "]\n",
        "                                                                 \n",
        "\n",
        "lgb_params = {\n",
        "                    'objective': 'multiclass',\n",
        "                    'metric': 'multi_logloss',\n",
        "                    'num_classes': 5,\n",
        "                } "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1inoOs4wXtb4"
      },
      "source": [
        "TOTAL_COST = 0.015 # %\n",
        "num_periods_annually = 251 # количество торговых дней в году\n",
        "risk_free_rate = 0.02/12 # безрисковая месячная ставка\n",
        "TARGET = 'target'\n",
        "BETA = 0\n",
        "TICKERS_TO_TRADE = 40 ### сколько тикеров покупаем каждый месяц\n",
        "years_to_train = 15 ### протестировано и выбрано\n",
        "first_year = pd.DatetimeIndex([sorted(quotes.date.unique())[0]]).year[0]\n",
        "last_year = first_year + years_to_train\n",
        "\n",
        "CAPITAL = 1000000\n",
        "START_CAPITAL = CAPITAL\n",
        "CAPITAL_HISTORY = {np.datetime64(datetime.datetime(last_year,4,30)): CAPITAL} \n",
        "\n",
        "quotes['volume_USD'] = quotes['close']*quotes['volume'] # фильтр только на ликвидные акции \n",
        "VOLUME_TRESHOLD = 5000 # фильтр только на ликвидные акции c оборотом больше 5k долларов в посдедний день месяца\n",
        "\n",
        "while last_year <= 2021-1:\n",
        "  START_TRAIN = datetime.datetime(first_year,4,30) \n",
        "  END_TRAIN = datetime.datetime(last_year,4,30)\n",
        "  END_TEST = datetime.datetime(last_year+1,4,30)\n",
        "  print('TRAIN END:', last_year, 'TRAIN START:', last_year - years_to_train, 'TEST PERIOD:', last_year+1)\n",
        "  first_year += 1\n",
        "  last_year += 1\n",
        "  ##########################################################\n",
        "  train_mask = (quotes['date']>=START_TRAIN)&(quotes['date']<=END_TRAIN)&(-quotes['symbol'].isin(skip_symbols))\n",
        "  test_mask = (quotes['date']>END_TRAIN)&(quotes['date']<=END_TEST)&(-quotes['symbol'].isin(skip_symbols))&(quotes['volume_USD']>VOLUME_TRESHOLD)\n",
        "\n",
        "  train_data_lgb = lgb.Dataset(quotes[train_mask][features_columns], label=quotes[train_mask][TARGET])\n",
        "  test_data_lgb = lgb.Dataset(quotes[test_mask][features_columns], label=quotes[test_mask][TARGET])\n",
        "\n",
        "  estimator = lgb.train(lgb_params, \n",
        "                        train_data_lgb,\n",
        "                        valid_sets = [train_data_lgb, test_data_lgb],\n",
        "                        verbose_eval = 200)\n",
        "  \n",
        "  test_data = quotes[test_mask]\n",
        "  test_dates = [date for date in list(test_data[test_mask]['date'].sort_values().unique())] # определяем список дат на которые нужно тест провести\n",
        "  nasdaq = test_data[['date','NASDAQCOM']].drop_duplicates().sort_values(by=['date']).set_index('date').dropna() # benchmark\n",
        "  nasdaq['nasdaq_pct_change'] = nasdaq['NASDAQCOM'].astype(float).pct_change().shift(-1) # benchmark\n",
        "  nasdaq = nasdaq.reset_index() # benchmark\n",
        "\n",
        "  for date in test_dates:\n",
        "    # делаем срез для торговой даты\n",
        "    test_data['volume_USD'] = test_data['close']*test_data['volume']\n",
        "    sub_df = test_data[(test_data['date']==date)&(test_data['volume_USD']>VOLUME_TRESHOLD)] #\n",
        "    sub_nasdaq = nasdaq[nasdaq['date']==date] # делаем срез по насдаку чтобы узнать изменение за следующий месяц по нему\n",
        "\n",
        "    # делаем предсказание для торговой даты\n",
        "    result = estimator.predict(sub_df[features_columns])\n",
        "    # вывод выбранной категории в отдельный столбец\n",
        "    final_result = []\n",
        "    for line in result:\n",
        "      final_result.append(list(line).index(max(line)))\n",
        "    sub_df['prediction'] = final_result\n",
        "    # и мэпим вероятности классов для каждого тикера в этот день\n",
        "    sub_df.loc[:,'prob_0'] = [sublist[0] for sublist in result]\n",
        "    sub_df.loc[:,'prob_1'] = [sublist[1] for sublist in result]\n",
        "    sub_df.loc[:,'prob_2'] = [sublist[2] for sublist in result]\n",
        "    sub_df.loc[:,'prob_3'] = [sublist[3] for sublist in result]\n",
        "    sub_df.loc[:,'prob_4'] = [sublist[4] for sublist in result]\n",
        "\n",
        "    # делаем столбец где отмечаем торговать или нет\n",
        "    sub_df['is_trade_long'] = sub_df['prediction']==0\n",
        "    # sub_df['is_trade_short'] = sub_df['prediction']==4\n",
        "\n",
        "    mask_long = sub_df['is_trade_long']==True # отбираем все строки где стратегия помэпила лонг\n",
        "    symbols_long = sub_df[mask_long][['symbol', 'is_trade_long', 'prob_0']].sort_values(by=['prob_0']).tail(TICKERS_TO_TRADE)['symbol'].values # отбираем 40 акций с наибольшей верояютностью к росту\n",
        "    long_mask = sub_df['symbol'].isin(symbols_long)\n",
        "\n",
        "    if len(symbols_long)>5:\n",
        "      res_long = sub_df['price_change_next_calendar_month'][long_mask].mean()-TOTAL_COST\n",
        "    else:\n",
        "      # принимаем решение не торговать, а вкладываем в безрисковый процент\n",
        "      res_long = risk_free_rate\n",
        "    \n",
        "    res = res_long\n",
        "\n",
        "    CAPITAL = CAPITAL * (1+res_long)\n",
        "    CAPITAL_HISTORY[date] = CAPITAL\n",
        "\n",
        "# Сохраняем самую последнюю обученную модель\n",
        "with open('{}lgbm_estimator.pickle'.format(DIRECTORY), 'wb') as f:\n",
        "  pickle.dump(estimator, f)\n",
        "\n",
        "# сделали прогон, теперь считаем метрики по получившемуся портфелю\n",
        "portfolio_var = np.std(pd.DataFrame(CAPITAL_HISTORY.values()).pct_change())\n",
        "df = pd.DataFrame(CAPITAL_HISTORY.values()).pct_change()\n",
        "df = list(df[df.values<0][0])\n",
        "negative_strategie_var = np.std(df)\n",
        "portfolio_profit = (pd.DataFrame(CAPITAL_HISTORY.values()).pct_change()).mean()\n",
        "\n",
        "STRATEGY_SHARP = round(float(portfolio_profit/portfolio_var)*math.sqrt(12),3) # Коэффициент шарпа годовой\n",
        "STRATEGY_RESULT = (CAPITAL - START_CAPITAL) / START_CAPITAL # во сколько раз вырос депозит \n",
        "STRATEGY_VAR =  round(float(portfolio_var),3) # VAR депозита\n",
        "STRATEGY_BETA = cal_beta(pd.DataFrame(list(CAPITAL_HISTORY.values()), index=CAPITAL_HISTORY.keys())) # считаем бетту с насдак композит\n",
        "\n",
        "print('VAR', round(float(portfolio_var),3), \n",
        "      'PROFIT',  round(float(portfolio_profit),3),\n",
        "      'SHARP', round(float(portfolio_profit/portfolio_var)*math.sqrt(12),3),\n",
        "      'RESULT', round(STRATEGY_RESULT,2),\n",
        "      'BETA', round(STRATEGY_BETA, 2))\n",
        "\n",
        "\n",
        "# рисуем графики\n",
        "df = pd.DataFrame(list(CAPITAL_HISTORY.values()), index=CAPITAL_HISTORY.keys())\n",
        "window = 252\n",
        "Roll_Max = df[0].rolling(window, min_periods=1).max()\n",
        "Daily_Drawdown = df[0]/Roll_Max - 1.0\n",
        "Max_Daily_Drawdown = Daily_Drawdown.rolling(window, min_periods=1).min()\n",
        "\n",
        "fig, ax = plt.subplots(2, figsize=(10,7), sharex=True)\n",
        "ax[0].plot(np.log(pd.DataFrame(list(CAPITAL_HISTORY.values()), index=CAPITAL_HISTORY.keys())),linewidth=1) \n",
        "fig.suptitle('Result and Drawdown, END TRAIN YEAR: {}, VAR {}, PROFIT {}, SHARP {}, BETA {}'.format(END_TRAIN, \n",
        "                                                                                            round(float(portfolio_var),3), \n",
        "                                                                                            round(float(portfolio_profit),3), \n",
        "                                                                                            round(float(portfolio_profit/portfolio_var)*math.sqrt(12),3),\n",
        "                                                                                            round(STRATEGY_BETA, 2)))\n",
        "\n",
        "ax[1].plot(Daily_Drawdown)\n",
        "ax[1].plot(Max_Daily_Drawdown)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbRn3S0FYKTl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpP0cXHcbAJp"
      },
      "source": [
        "# Для того, чтобы сделать предсказание на последнюю доступную дату, надо подготовить датасет. Здесь загружается датасет подготовленный для реал тайм предсказания\n",
        "with open('{}final_df_last_year.pickle'.format(DIRECTORY), 'rb') as f: \n",
        "  quotes_final = pickle.load(f)\n",
        "\n",
        "with open('{}lgbm_estimator.pickle'.format(DIRECTORY), 'rb') as f: \n",
        "  estimator = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTgFS0TrbfO7"
      },
      "source": [
        "# делаем срез для торговой даты\n",
        "DATE_TO_TRADE = '2021-05-28'\n",
        "VOLUME_TRESHOLD = 5000 # фильтр только на ликвидные акции c оборотом больше 5k долларов в посдедний день месяца чтобы отсечь совсем неликвид\n",
        "TICKERS_TO_TRADE = 40 # сколько акций набираем в портфель\n",
        "\n",
        "sub_df = quotes_final[quotes_final['date']==DATE_TO_TRADE]\n",
        "\n",
        "# делаем предсказание для торговой даты\n",
        "result = estimator.predict(sub_df[features_columns])\n",
        "# вывод выбранной категории в отдельный столбец\n",
        "final_result = []\n",
        "for line in result:\n",
        "  final_result.append(list(line).index(max(line)))\n",
        "sub_df['prediction'] = final_result\n",
        "# и мэпим вероятности классов для каждого тикера в этот день\n",
        "sub_df.loc[:,'prob_0'] = [sublist[0] for sublist in result]\n",
        "sub_df.loc[:,'prob_1'] = [sublist[1] for sublist in result]\n",
        "sub_df.loc[:,'prob_2'] = [sublist[2] for sublist in result]\n",
        "sub_df.loc[:,'prob_3'] = [sublist[3] for sublist in result]\n",
        "sub_df.loc[:,'prob_4'] = [sublist[4] for sublist in result]\n",
        "\n",
        "sub_df['is_trade_long'] = sub_df['prediction']==0\n",
        "\n",
        "mask_long = sub_df['is_trade_long']==True # отбираем все строки где стратегия помэпила лонг\n",
        "mask_volume = sub_df['volume']*sub_df['close']>VOLUME_TRESHOLD # только ликвидные акции с оборотм больше 5к долларов в день\n",
        "symbols_long = sub_df[mask_long&mask_volume][['symbol', 'is_trade_long', 'prob_0']].sort_values(by=['prob_0']).tail(TICKERS_TO_TRADE)['symbol'].values # отбираем 40 акций с наибольшей верояютностью к росту \n",
        "symbols_long # показать все отобранные тикеры"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIvZtSufcVfZ"
      },
      "source": [
        "## Реал тайм тестирование стрегии, выбор тикеров на июнь\n",
        "\n",
        "Ниже название тикера и цена открытия - клоуз 1 июня\n",
        "\n",
        "TICKER: CLOSE AT 01/06/21\n",
        "\n",
        "CXDC: 0.64\n",
        "\n",
        "VMEO: 42.84\n",
        "\n",
        "BCYPW: 0.67\n",
        "\n",
        "HCICW: 1.2\n",
        "\n",
        "BIMI: 1.32\n",
        "\n",
        "ODT: 3.66\n",
        "\n",
        "ACAD: 22.35\n",
        "\n",
        "YJ: 1.82\n",
        "\n",
        "FAMI: 0.44\n",
        "\n",
        "EBON: 3.41\n",
        "\n",
        "NRBO: 2.88\n",
        "\n",
        "PAQCW: 0.69\n",
        "\n",
        "SMID: 18.97\n",
        "\n",
        "MGI: 9.7\n",
        "\n",
        "RAACW: 1.47\n",
        "\n",
        "DCRNW: 1.37\n",
        "\n",
        "BCEL: 9.18\n",
        "\n",
        "LYRA: 7.63\n",
        "\n",
        "ALACW: 0.3181\n",
        "\n",
        "IOVA: 18.27\n",
        "\n",
        "GIFI: 4.92\n",
        "\n",
        "SGTX: 11.71\n",
        "\n",
        "PSTI: 3.82\n",
        "\n",
        "ZVO: 2.31\n",
        "\n",
        "LOTZ: 4.98\n",
        "\n",
        "METX: 0.97\n",
        "\n",
        "RELI: 3.98\n",
        "\n",
        "DCPH: 33.56\n",
        "\n",
        "NKLA: 15.44\n",
        "\n",
        "EVFM: 0.897\n",
        "\n",
        "PROG: 2.86\n",
        "\n",
        "EVOK: 1.36\n",
        "\n",
        "ICPT: 17.16\n",
        "\n",
        "EZGO: 4.17\n",
        "\n",
        "AEGN: 5.5\n",
        "\n",
        "HYMCW: 0.63\n",
        "\n",
        "SEII: 0.03\n",
        "\n",
        "DUO: 3.3\n",
        "\n",
        "QK: 0.992\n",
        "\n",
        "NBRV: 1.39\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL4aIbnRcOZH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}